{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Elevate: model-enhanced LLM driven VUI testing on VPA apps Files Tree \u251c\u2500\u2500 code \u2502 \u251c\u2500\u2500 main.py \u2502 \u251c\u2500\u2500 step1_process_document.py \u2502 \u251c\u2500\u2500 step2_test_skill.py \u2502 \u251c\u2500\u2500 step3_detect_problem.py \u2502 \u251c\u2500\u2500 model \u2502 \u2502 \u2514\u2500\u2500 ... \u2502 \u251c\u2500\u2500 skill \u2502 \u2502 \u2514\u2500\u2500 ... \u2502 \u2514\u2500\u2500 util \u2502 \u2514\u2500\u2500 ... \u251c\u2500\u2500 chrome \u2502 \u2514\u2500\u2500 chromedriver_new.exe \u251c\u2500\u2500 config \u2502 \u2514\u2500\u2500 config000.ini \u251c\u2500\u2500 cookie \u2502 \u2514\u2500\u2500 ... \u251c\u2500\u2500 corpus \u2502 \u2514\u2500\u2500 ... \u251c\u2500\u2500 dataset_2022 \u2502 \u251c\u2500\u2500 benchmark.xlsx \u2502 \u2514\u2500\u2500 large_scale_4000.xlsx \u251c\u2500\u2500 README.md \u2514\u2500\u2500 requirement.txt code: contains the source code of Elevate chrome: contains the current chromedriver on windows (24th, March, 2024) config: configuration files. dataset_2022: benchmark skills and large-scale skills requirement.txt: requirements of python packages Requirement python package pip install -r requirement.txt python -m spacy download en_core_web_sm environment export OPENAI_API_KEY=<YOUR OPENAI API KEY> Other the chromedriver that matches your chrome version an amazon developer account that can use the simulator (change the config000.ini in config directory to add your acount) an Azure account that can access to the openAI API (change the config000.ini in config directory to add the apibase and apiversion) We need to login to the amazon developer account to start using the simulator. In most of the cases amazon will send an email to your linked email address for verifications. We use pop to read the emails, so make sure the 110 port is open. How to run Elevate cd code python main.py -i <begin index of skill, default as 1> -ei <end index of skill, default as 100> -c <configuration file name in the config directory, default as config000.ini> -e <input skills file in the dataset_2022 directory, default as benchmark.xlsx> -l <path to save communication logs and results, default as ../../output/benchmark> -d <chromedriver name in the chrome directory, default as chromedriver_new.exe> Download Elevate See the Elevate directory for running Elevate. Dataset can also be found here. Output & Experiment The data for experiments can all be found here. Approaches to run the experiments can be found here .","title":"Home"},{"location":"#elevate-model-enhanced-llm-driven-vui-testing-on-vpa-apps","text":"","title":"Elevate: model-enhanced LLM driven VUI testing on VPA apps"},{"location":"#files-tree","text":"\u251c\u2500\u2500 code \u2502 \u251c\u2500\u2500 main.py \u2502 \u251c\u2500\u2500 step1_process_document.py \u2502 \u251c\u2500\u2500 step2_test_skill.py \u2502 \u251c\u2500\u2500 step3_detect_problem.py \u2502 \u251c\u2500\u2500 model \u2502 \u2502 \u2514\u2500\u2500 ... \u2502 \u251c\u2500\u2500 skill \u2502 \u2502 \u2514\u2500\u2500 ... \u2502 \u2514\u2500\u2500 util \u2502 \u2514\u2500\u2500 ... \u251c\u2500\u2500 chrome \u2502 \u2514\u2500\u2500 chromedriver_new.exe \u251c\u2500\u2500 config \u2502 \u2514\u2500\u2500 config000.ini \u251c\u2500\u2500 cookie \u2502 \u2514\u2500\u2500 ... \u251c\u2500\u2500 corpus \u2502 \u2514\u2500\u2500 ... \u251c\u2500\u2500 dataset_2022 \u2502 \u251c\u2500\u2500 benchmark.xlsx \u2502 \u2514\u2500\u2500 large_scale_4000.xlsx \u251c\u2500\u2500 README.md \u2514\u2500\u2500 requirement.txt code: contains the source code of Elevate chrome: contains the current chromedriver on windows (24th, March, 2024) config: configuration files. dataset_2022: benchmark skills and large-scale skills requirement.txt: requirements of python packages","title":"Files Tree"},{"location":"#requirement","text":"","title":"Requirement"},{"location":"#python-package","text":"pip install -r requirement.txt python -m spacy download en_core_web_sm","title":"python package"},{"location":"#environment","text":"export OPENAI_API_KEY=<YOUR OPENAI API KEY>","title":"environment"},{"location":"#other","text":"the chromedriver that matches your chrome version an amazon developer account that can use the simulator (change the config000.ini in config directory to add your acount) an Azure account that can access to the openAI API (change the config000.ini in config directory to add the apibase and apiversion) We need to login to the amazon developer account to start using the simulator. In most of the cases amazon will send an email to your linked email address for verifications. We use pop to read the emails, so make sure the 110 port is open.","title":"Other"},{"location":"#how-to-run-elevate","text":"cd code python main.py -i <begin index of skill, default as 1> -ei <end index of skill, default as 100> -c <configuration file name in the config directory, default as config000.ini> -e <input skills file in the dataset_2022 directory, default as benchmark.xlsx> -l <path to save communication logs and results, default as ../../output/benchmark> -d <chromedriver name in the chrome directory, default as chromedriver_new.exe>","title":"How to run Elevate"},{"location":"#download","text":"Elevate See the Elevate directory for running Elevate. Dataset can also be found here. Output & Experiment The data for experiments can all be found here. Approaches to run the experiments can be found here .","title":"Download"},{"location":"case_study/","text":"Case study In this case study, we select four real world cases of VPA apps. The first two cases reflect the effectiveness of Elevate's agent on problems detection and state space coverage. The last two cases show the limitation of Elevate. Case 1 Generator generates diverse and context-related inputs which benefit new states coverage and problems detection This is an Alexa skill called My Horoscope. The skill's main functionality is to show insights and forecasts. Here is a piece of communication log between My Horoscope and Elevate. ... Skill: My Horoscope provides your daily horoscope along with insights into love, career, and money. For example, to get the horoscope for Capricorn, say, \"Alexa, what is the horoscope for Capricorn?\". To change your default sign, say, \"Alexa, change my default zodiac sign.\". Which zodiac sign your need the horoscope for? Elevate: alexa, change my default zodiac sign. Skill: Which zodiac sign do you want to set as default? Elevate: aries Skill: Oh an Aries, a beautiful sign for a beautiful soul....Would you like to start your sevenday free trial? Elevate: yes Alexa: Sorry, I am unable to fulfill your request on this device. <unexpected exit> This is the corresponding communication log between My Horoscope and Vitas. ... Skill: My Horoscope provides your daily horoscope along with insights into love, career, and money. For example, to get the horoscope for Capricorn, say, \"Alexa, what is the horoscope for Capricorn?\". To change your default sign, say, \"Alexa, change my default zodiac sign.\". Which zodiac sign your need the horoscope for? Vitas: change my default zodiac sign. Skill: Which zodiac sign do you want to set as default? Vitas: transform Skill: Sorry. I missed that....What sign do you want the horoscope for? Vitas: confidence Skill: Sorry. I missed that....What sign do you want the horoscope for? Vitas: transform Skill: Sorry. I missed that....What sign do you want the horoscope for? Vitas: reinforces Skill: Sorry. I missed that....What sign do you want the horoscope for? ... In this case, Vitas stucks on the state \"What sign do you want the horoscope for?\" because it cannot generate context-related inputs. Consequently, Vitas cannot discover new states or problems after that. As a comparison, the Generator agent generates several context-related inputs ([\"Aries\", \"Taurus\", \"Gemini\", \"Cancer\", \"Leo\", \"Virgo\", \"Libra\", \"Scorpio\", \"Sagittarius\", \"Capricorn\", \"Aquarius\", \"Pisces\"]) for this state , and the new state \"Would you like to start your sevenday free trial?\" is discovered. After that, the unexpected exit problem is found. Case 2 Observer merges semantic similar outputs to avoid repeated tests, while Planner understands semantics to choose the best input This is an Alexa skill called Action movies. The skill's main functionality is to recommend different movies. Here is a piece of communication logs between Action movies and Elevate. ... Skill: You can ask me to recommend a movie answering yes or no. Do you want me to recommend a movie? Elevate: yes Skill: Terminator. I tell you another movie. Elevate: yes Skill: Terminator. I tell you another movie. Elevate: action Skill: James Bond. I tell you another one. Elevate: title Alexa: I missed something. Can you say it again. <unexpected exit> This is the corresponding communication log between Action movies and Vitas. ... Skill: You can ask me to recommend a movie answering yes or no. Do you want me to recommend a movie? Vitas: recommend a movie answering yes Skill: Black Panther. I tell you another movie. Vitas: recommend a movie answering yes Skill: Rambo. I tell you another one. Vitas: no Skill: Bye. <Skill exits normally> ... Skill: Total Recall. I tell you another movie. Vitas: recommend a movie answering yes Skill: Die Hard. I tell you another one. ... In this case, Observer merges \"I tell you another movie.\" and \"I tell you another one\" as the same semantic state. Planner learns from the behavior model that the inputs \"yes\" and \"action\" cannot help discover new states or new problems. Also, Planner understands the potential results of the input \"no\", which is to end the communication. Finally, it chose \"title\" as input when visiting the same state the third times. After that, a new unexpected exit problem is found. As a comparison, Vitas considered \"I tell you another movie.\" and \"I tell you another one\" as two different states. So \"recommend a movie answering yes\" was considered as a good input as it helped discover new states. We can see from the log that \"recommend a movie answering yes\" was chosen three times, which means that the same functionality was tested three times. Despite that, Vitas chose the input without considering semantic relevance. That is the reason why it chose \"no\" as the input and the communication ended. This case proves our assumptions. Firstly, ignoring sematic relevance when extracting states will affect the testing efficiency. This is the motivation of designing Observer . Secondly, semantic relevance is important when choosing inputs. This drawback is handled by Planner , who considers context-relevance and history testing results when making testing decisions. Case 3 Speeches in audio files are not adequately analyzed Elevate runs on the simulator, whose outputs are mostly in the form of text. However, in rare circumstances, the simulator returns an audio file and outputs the text <Short audio>. Both Elevate and Vitas do not parse the audio files, but these files do contain information such as speeches or music. For example, a skill called Butterball only returns an audio file after launching. The log looks like: Elevate/Vitas: Alexa open butterball Skill: <Short audio>. Elevate/Vitas: help Actually, in the audio file, it says \"Welcome back to butterball. I'm Beth. We can help you with planning, preparing, cooking or enjoying a turkey. What can I help with?\". As Elevate and Vitas do not parse the audio file for text information, the context-related inputs [\"planning a turkey\", \"preparing a turkey\", \"cooking a turkey\", \"enjoying a turkey\"] cannot be generated. Meanwhile, Observer in Elevate and Vitas merge all audio files into one state because they are all in the text form <Short audio>. As a result, the information embeded in the audio file is not adequately used, resulting in insufficient testing of subsequent behaviors. Case 4 Music in audio files is not parsed Another example is a skill called Song Quiz. Its functionality is to randomly play music and ask users to guess the title and artist. The music is uploaded in an audio file. The simulator plays the audio file and returns the <Short audio> text. There is the communication log betweeen Song Quiz and testers. Skill: Starting your 60s game. Your opponent is Sam from Edinburgh. I'll play 5 short clips. Guess the song title or artist, or you can guess both for bonus points. Question 1 , for 10 points. Name the song title and artist:.<Short audio>. Elevate: shape of you by ed sheeran / Vitas: artist In such cases, testers must understand the music to generate inputs, which is difficult for current VPA apps testers to handle. Although Elevate cannot analyze the audio file to get the correct answer, it can learn from the context to acquire the domain of answers. Consequently, its input is related to the song title and artist, although it is not the right one. As a comparison, Vitas just inputs \"artist\", which is not a valid answer.","title":"Case study"},{"location":"case_study/#case-study","text":"In this case study, we select four real world cases of VPA apps. The first two cases reflect the effectiveness of Elevate's agent on problems detection and state space coverage. The last two cases show the limitation of Elevate.","title":"Case study"},{"location":"case_study/#case-1","text":"","title":"Case 1"},{"location":"case_study/#generator-generates-diverse-and-context-related-inputs-which-benefit-new-states-coverage-and-problems-detection","text":"This is an Alexa skill called My Horoscope. The skill's main functionality is to show insights and forecasts. Here is a piece of communication log between My Horoscope and Elevate. ... Skill: My Horoscope provides your daily horoscope along with insights into love, career, and money. For example, to get the horoscope for Capricorn, say, \"Alexa, what is the horoscope for Capricorn?\". To change your default sign, say, \"Alexa, change my default zodiac sign.\". Which zodiac sign your need the horoscope for? Elevate: alexa, change my default zodiac sign. Skill: Which zodiac sign do you want to set as default? Elevate: aries Skill: Oh an Aries, a beautiful sign for a beautiful soul....Would you like to start your sevenday free trial? Elevate: yes Alexa: Sorry, I am unable to fulfill your request on this device. <unexpected exit> This is the corresponding communication log between My Horoscope and Vitas. ... Skill: My Horoscope provides your daily horoscope along with insights into love, career, and money. For example, to get the horoscope for Capricorn, say, \"Alexa, what is the horoscope for Capricorn?\". To change your default sign, say, \"Alexa, change my default zodiac sign.\". Which zodiac sign your need the horoscope for? Vitas: change my default zodiac sign. Skill: Which zodiac sign do you want to set as default? Vitas: transform Skill: Sorry. I missed that....What sign do you want the horoscope for? Vitas: confidence Skill: Sorry. I missed that....What sign do you want the horoscope for? Vitas: transform Skill: Sorry. I missed that....What sign do you want the horoscope for? Vitas: reinforces Skill: Sorry. I missed that....What sign do you want the horoscope for? ... In this case, Vitas stucks on the state \"What sign do you want the horoscope for?\" because it cannot generate context-related inputs. Consequently, Vitas cannot discover new states or problems after that. As a comparison, the Generator agent generates several context-related inputs ([\"Aries\", \"Taurus\", \"Gemini\", \"Cancer\", \"Leo\", \"Virgo\", \"Libra\", \"Scorpio\", \"Sagittarius\", \"Capricorn\", \"Aquarius\", \"Pisces\"]) for this state , and the new state \"Would you like to start your sevenday free trial?\" is discovered. After that, the unexpected exit problem is found.","title":"Generator generates diverse and context-related inputs which benefit new states coverage and problems detection"},{"location":"case_study/#case-2","text":"","title":"Case 2"},{"location":"case_study/#observer-merges-semantic-similar-outputs-to-avoid-repeated-tests-while-planner-understands-semantics-to-choose-the-best-input","text":"This is an Alexa skill called Action movies. The skill's main functionality is to recommend different movies. Here is a piece of communication logs between Action movies and Elevate. ... Skill: You can ask me to recommend a movie answering yes or no. Do you want me to recommend a movie? Elevate: yes Skill: Terminator. I tell you another movie. Elevate: yes Skill: Terminator. I tell you another movie. Elevate: action Skill: James Bond. I tell you another one. Elevate: title Alexa: I missed something. Can you say it again. <unexpected exit> This is the corresponding communication log between Action movies and Vitas. ... Skill: You can ask me to recommend a movie answering yes or no. Do you want me to recommend a movie? Vitas: recommend a movie answering yes Skill: Black Panther. I tell you another movie. Vitas: recommend a movie answering yes Skill: Rambo. I tell you another one. Vitas: no Skill: Bye. <Skill exits normally> ... Skill: Total Recall. I tell you another movie. Vitas: recommend a movie answering yes Skill: Die Hard. I tell you another one. ... In this case, Observer merges \"I tell you another movie.\" and \"I tell you another one\" as the same semantic state. Planner learns from the behavior model that the inputs \"yes\" and \"action\" cannot help discover new states or new problems. Also, Planner understands the potential results of the input \"no\", which is to end the communication. Finally, it chose \"title\" as input when visiting the same state the third times. After that, a new unexpected exit problem is found. As a comparison, Vitas considered \"I tell you another movie.\" and \"I tell you another one\" as two different states. So \"recommend a movie answering yes\" was considered as a good input as it helped discover new states. We can see from the log that \"recommend a movie answering yes\" was chosen three times, which means that the same functionality was tested three times. Despite that, Vitas chose the input without considering semantic relevance. That is the reason why it chose \"no\" as the input and the communication ended. This case proves our assumptions. Firstly, ignoring sematic relevance when extracting states will affect the testing efficiency. This is the motivation of designing Observer . Secondly, semantic relevance is important when choosing inputs. This drawback is handled by Planner , who considers context-relevance and history testing results when making testing decisions.","title":"Observer merges semantic similar outputs to avoid repeated tests, while Planner understands semantics to choose the best input"},{"location":"case_study/#case-3","text":"","title":"Case 3"},{"location":"case_study/#speeches-in-audio-files-are-not-adequately-analyzed","text":"Elevate runs on the simulator, whose outputs are mostly in the form of text. However, in rare circumstances, the simulator returns an audio file and outputs the text <Short audio>. Both Elevate and Vitas do not parse the audio files, but these files do contain information such as speeches or music. For example, a skill called Butterball only returns an audio file after launching. The log looks like: Elevate/Vitas: Alexa open butterball Skill: <Short audio>. Elevate/Vitas: help Actually, in the audio file, it says \"Welcome back to butterball. I'm Beth. We can help you with planning, preparing, cooking or enjoying a turkey. What can I help with?\". As Elevate and Vitas do not parse the audio file for text information, the context-related inputs [\"planning a turkey\", \"preparing a turkey\", \"cooking a turkey\", \"enjoying a turkey\"] cannot be generated. Meanwhile, Observer in Elevate and Vitas merge all audio files into one state because they are all in the text form <Short audio>. As a result, the information embeded in the audio file is not adequately used, resulting in insufficient testing of subsequent behaviors.","title":"Speeches in audio files are not adequately analyzed"},{"location":"case_study/#case-4","text":"","title":"Case 4"},{"location":"case_study/#music-in-audio-files-is-not-parsed","text":"Another example is a skill called Song Quiz. Its functionality is to randomly play music and ask users to guess the title and artist. The music is uploaded in an audio file. The simulator plays the audio file and returns the <Short audio> text. There is the communication log betweeen Song Quiz and testers. Skill: Starting your 60s game. Your opponent is Sam from Edinburgh. I'll play 5 short clips. Guess the song title or artist, or you can guess both for bonus points. Question 1 , for 10 points. Name the song title and artist:.<Short audio>. Elevate: shape of you by ed sheeran / Vitas: artist In such cases, testers must understand the music to generate inputs, which is difficult for current VPA apps testers to handle. Although Elevate cannot analyze the audio file to get the correct answer, it can learn from the context to acquire the domain of answers. Consequently, its input is related to the song title and artist, although it is not the right one. As a comparison, Vitas just inputs \"artist\", which is not a valid answer.","title":"Music in audio files is not parsed"},{"location":"case_study1/","text":"Case study In this case study, we select three real world cases of VPA apps. Each of the cases demonstrates the effectiveness of Elevate's one agent. Case 1 This is an Alexa skill called Action movies. The skill's main functionality is to recommend different movies. Here is a piece of communication logs between Action movies and Elevate. Skill: Ok, Here's Action movies.Welcome to action movies. Do you want me to tell you a movie? Elevate: help Skill: You can ask me to recommend a movie answering yes or no. Do you want me to recommend a movie? Elevate: yes ... Skill: Welcome to action movies. Do you want me to tell you a movie? Elevate: no Below shows the recording of Observer 's prompts and outputs related to the above communication log. Only *Short Prompt* is displayed. Short Prompt: Input: sentence: \"Do you want me to tell you a movie?\", state list: ['<START>'] Output: Observer: Do you want me to tell you a movie? Short Prompt: Input: sentence: \"Do you want me to recommend a movie?\", state list: ['<START>', 'Do you want me to tell you a movie?'] Output: Observer: Do you want me to tell you a movie? In this case, \"Do you want me to tell you a movie?\" and \"Do you want me to recommend a movie?\" are semantically similar outputs. Observer correctly maps the output \"Do you want me to recommend a movie?\" to an existing state \"Do you want me to tell you a movie?\". Therefore, Elevate recognizes the three skill's outputs as one state, so it uses different input events to efficiently trigger skill's different functionalities. Case 2 This is an Alexa skill called Age Calculator. The skill's main functionality is to calculate the number of days between two dates. Here is a piece of communication logs between Age Calculator and Elevate. Skill: Handing off to Age Calculator by sayak.Age Calculator. You can ask me to calculate the amount of time between now and a specific date.What date would you like to use? Elevate: tommorrow Skill: The length of time between now and xxx is about xxx. ... Below shows the recording of Generator 's prompts and outputs related to the above communication log. Only *Short Prompt* is displayed. Short Prompt: Input: skill: \"What date would you like to use?\" Output: Generator: [\"today\", \"tomorrow\", \"July 4\", \"December 25\", \"next Monday\"] The output \"What date would you like to use?\" expects a specifc date as inputs. Otherwise, this skill will return information like \"I can only calculate based on exact dates.\". Without domain-specifc knowledge of correct \"date\", invalid test cases will be generated. However, most NLP based testers have difficulty in generating valid test cases in such scenarios. As a comparison, Generator correctly generates five valid dates, benefited from the LLM. Therefore, Elevate's testing based on such high quality test cases is much more effective. Case 3 This is an Alexa skill called Asthma Device Helper. The skill's main functionality is to provide usage instructions. Here is a piece of communication logs between Asthma Device Helper and Elevate. Skill: Welcome to Asthma Device Helper. You can ask a question like, what are the instructions for using a proair inhaler. ... Now, what can I help you with? Elevate: help Skill: You can ask questions such as, what's the instructions, or, you can say exit...Now, what can I help you with? Elevate: what are the instructions for using a proair inhaler Skill: When first using your proair inhaler... Below shows the recording of Planner 's prompts and outputs related to the above communication log. Short Prompt: Input: <current state>=\"You can ask questions such as, what's the instructions, or, you can say exit...\",FSM={\u03a3={\"what's the instructions\":0,\"exit\":0,\"what are the instructions for using a proair inhaler\":0,\"help\":1,\"pause\":0,\"resume\":0,\"stop\":0,\"what's the time\":0},\u03b4=([<current_state>,\"help\",<current_state>])}. Thought: Planner: ... Output: \"what are the instructions for using a proair inhaler\" The state \"You can ask questions such as, what's the instructions, or, you can say exit...\" has many candidate input events, including semantic relevant inputs like \"what's the instructions\", \"what are the instructions for using a proair inhaler\" and \"help\", and semantic irrelevant inputs like \"exit\", \"pause\", \"stop\", etc.. Planner considers the semantic relevance, history invocation times and transitions to make exploration decisions. Consequently, semantic irrelevant inputs have lower priority. Among semantic relevant inputs, \"help\" was chosen once and is invalid, so it is removed. From the left two choices, Planner selects \"what are the instructions for using a proair inhaler\" as the final input. Without considering semantic relevance, \"exit\" will have the same priority as \"what are the instructions for using a proair inhaler\" and \"what's the instructions\". If \"exit\" is selected, the skill will terminate, affecting the testing continuity.","title":"Case study"},{"location":"case_study1/#case-study","text":"In this case study, we select three real world cases of VPA apps. Each of the cases demonstrates the effectiveness of Elevate's one agent.","title":"Case study"},{"location":"case_study1/#case-1","text":"This is an Alexa skill called Action movies. The skill's main functionality is to recommend different movies. Here is a piece of communication logs between Action movies and Elevate. Skill: Ok, Here's Action movies.Welcome to action movies. Do you want me to tell you a movie? Elevate: help Skill: You can ask me to recommend a movie answering yes or no. Do you want me to recommend a movie? Elevate: yes ... Skill: Welcome to action movies. Do you want me to tell you a movie? Elevate: no Below shows the recording of Observer 's prompts and outputs related to the above communication log. Only *Short Prompt* is displayed. Short Prompt: Input: sentence: \"Do you want me to tell you a movie?\", state list: ['<START>'] Output: Observer: Do you want me to tell you a movie? Short Prompt: Input: sentence: \"Do you want me to recommend a movie?\", state list: ['<START>', 'Do you want me to tell you a movie?'] Output: Observer: Do you want me to tell you a movie? In this case, \"Do you want me to tell you a movie?\" and \"Do you want me to recommend a movie?\" are semantically similar outputs. Observer correctly maps the output \"Do you want me to recommend a movie?\" to an existing state \"Do you want me to tell you a movie?\". Therefore, Elevate recognizes the three skill's outputs as one state, so it uses different input events to efficiently trigger skill's different functionalities.","title":"Case 1"},{"location":"case_study1/#case-2","text":"This is an Alexa skill called Age Calculator. The skill's main functionality is to calculate the number of days between two dates. Here is a piece of communication logs between Age Calculator and Elevate. Skill: Handing off to Age Calculator by sayak.Age Calculator. You can ask me to calculate the amount of time between now and a specific date.What date would you like to use? Elevate: tommorrow Skill: The length of time between now and xxx is about xxx. ... Below shows the recording of Generator 's prompts and outputs related to the above communication log. Only *Short Prompt* is displayed. Short Prompt: Input: skill: \"What date would you like to use?\" Output: Generator: [\"today\", \"tomorrow\", \"July 4\", \"December 25\", \"next Monday\"] The output \"What date would you like to use?\" expects a specifc date as inputs. Otherwise, this skill will return information like \"I can only calculate based on exact dates.\". Without domain-specifc knowledge of correct \"date\", invalid test cases will be generated. However, most NLP based testers have difficulty in generating valid test cases in such scenarios. As a comparison, Generator correctly generates five valid dates, benefited from the LLM. Therefore, Elevate's testing based on such high quality test cases is much more effective.","title":"Case 2"},{"location":"case_study1/#case-3","text":"This is an Alexa skill called Asthma Device Helper. The skill's main functionality is to provide usage instructions. Here is a piece of communication logs between Asthma Device Helper and Elevate. Skill: Welcome to Asthma Device Helper. You can ask a question like, what are the instructions for using a proair inhaler. ... Now, what can I help you with? Elevate: help Skill: You can ask questions such as, what's the instructions, or, you can say exit...Now, what can I help you with? Elevate: what are the instructions for using a proair inhaler Skill: When first using your proair inhaler... Below shows the recording of Planner 's prompts and outputs related to the above communication log. Short Prompt: Input: <current state>=\"You can ask questions such as, what's the instructions, or, you can say exit...\",FSM={\u03a3={\"what's the instructions\":0,\"exit\":0,\"what are the instructions for using a proair inhaler\":0,\"help\":1,\"pause\":0,\"resume\":0,\"stop\":0,\"what's the time\":0},\u03b4=([<current_state>,\"help\",<current_state>])}. Thought: Planner: ... Output: \"what are the instructions for using a proair inhaler\" The state \"You can ask questions such as, what's the instructions, or, you can say exit...\" has many candidate input events, including semantic relevant inputs like \"what's the instructions\", \"what are the instructions for using a proair inhaler\" and \"help\", and semantic irrelevant inputs like \"exit\", \"pause\", \"stop\", etc.. Planner considers the semantic relevance, history invocation times and transitions to make exploration decisions. Consequently, semantic irrelevant inputs have lower priority. Among semantic relevant inputs, \"help\" was chosen once and is invalid, so it is removed. From the left two choices, Planner selects \"what are the instructions for using a proair inhaler\" as the final input. Without considering semantic relevance, \"exit\" will have the same priority as \"what are the instructions for using a proair inhaler\" and \"what's the instructions\". If \"exit\" is selected, the skill will terminate, affecting the testing continuity.","title":"Case 3"},{"location":"coverage/","text":"Coverage metric Why using the semantic state coverage rate as metric? In Study 1, we compare the average semantic state coverage rate with varying interaction rounds between Elevate and baselines. Semantic states are acquired by merging sentences with similar functionalities or behavior. Hence, the number of semantic states represents skill's distinct functionalities. Using semanic state coverage rate as a metric can better reflect testers' testing sufficiency. Why using Observer rather than human inspection as the ground-truth of semantic states? We calculate the accuracy of Observer in extracting semantic states on the benchmark. The accuracy for Observer's initial classfication is 88.1%. After the second check of the state filter, the accuracy increases to 97.5%. As it costs much labour and time to get the ground-truth of 4000 apps's semantic states, we use the semantic states classified by Observer as the metric. Other metric: Sentence state To demonstrate that Elevate outperforms baselines under different metrics, we compare the average sentence state coverage rate with varying interaction rounds in this study. The total state space is set as the unique sentence states achieved by Elevate and baselines. The result is shown below. Under the metric of sentence states, Elevate still achieves around 15% higher average coverage rate than Vitas in a more efficient manner. Total state space: including states of manual testing In our paper, the total state space is set as the unique semantic states achieved by Elevate and baselines. To get a more accurate total state space, we include the state space achieved by manual testing in this study. Then, the semantic state coverage rate with varying interaction rounds is compared between Elevate and baselines. The result is shown below. When manual testing results are considered in the total state space, only the denominator to calculate the semantic state coverage rate is changed. The performance of testers is almost the same as that shown in the study 1. Elevage still achieves an average coverage rate of nearly 70% and supasses baselines. Analyze the feature of state space achieved by Elevate, Vitas and Manual testing If we take the manual testing results as a comparison, we can learn the similarity of Elevate and Vitas's behavior with the human tester. We draw the venn diagram to show the intersection state space between Elevate, Vitas and Manual testing. The text on each part represents the state space coverage rate when the unique states achieved by Elevate, Vitas and Manual testing is set as the total space. The result is shown below. Compared with Vitas, Elevate covers more state space discovered by manual testing, which proves that Elevate works more like a manual tester. This result is in line with our expectations, as Elevate's three agents commit to complementing the semantic loss during VUI testing. To further analyze the characteristics of states achieved by Elevate and Vitas, we extract meaningful states, which represent the normal behavior of skills. Meaningful states are extracted manually from the states achieved by Elevate and Vitas. The venn diagram of meaningful states coverage by Elevate and Vitas is shown below. The figure demonstrate that Elevate achieves more meaningful states than Vitas. It proves that input events generated by Elevate are more effective in exploring apps' functionalities.","title":"Coverage metric"},{"location":"coverage/#coverage-metric","text":"","title":"Coverage metric"},{"location":"coverage/#why-using-the-semantic-state-coverage-rate-as-metric","text":"In Study 1, we compare the average semantic state coverage rate with varying interaction rounds between Elevate and baselines. Semantic states are acquired by merging sentences with similar functionalities or behavior. Hence, the number of semantic states represents skill's distinct functionalities. Using semanic state coverage rate as a metric can better reflect testers' testing sufficiency.","title":"Why using the semantic state coverage rate as metric?"},{"location":"coverage/#why-using-observer-rather-than-human-inspection-as-the-ground-truth-of-semantic-states","text":"We calculate the accuracy of Observer in extracting semantic states on the benchmark. The accuracy for Observer's initial classfication is 88.1%. After the second check of the state filter, the accuracy increases to 97.5%. As it costs much labour and time to get the ground-truth of 4000 apps's semantic states, we use the semantic states classified by Observer as the metric.","title":"Why using Observer rather than human inspection as the ground-truth of semantic states?"},{"location":"coverage/#other-metric-sentence-state","text":"To demonstrate that Elevate outperforms baselines under different metrics, we compare the average sentence state coverage rate with varying interaction rounds in this study. The total state space is set as the unique sentence states achieved by Elevate and baselines. The result is shown below. Under the metric of sentence states, Elevate still achieves around 15% higher average coverage rate than Vitas in a more efficient manner.","title":"Other metric: Sentence state"},{"location":"coverage/#total-state-space-including-states-of-manual-testing","text":"In our paper, the total state space is set as the unique semantic states achieved by Elevate and baselines. To get a more accurate total state space, we include the state space achieved by manual testing in this study. Then, the semantic state coverage rate with varying interaction rounds is compared between Elevate and baselines. The result is shown below. When manual testing results are considered in the total state space, only the denominator to calculate the semantic state coverage rate is changed. The performance of testers is almost the same as that shown in the study 1. Elevage still achieves an average coverage rate of nearly 70% and supasses baselines.","title":"Total state space: including states of manual testing"},{"location":"coverage/#analyze-the-feature-of-state-space-achieved-by-elevate-vitas-and-manual-testing","text":"If we take the manual testing results as a comparison, we can learn the similarity of Elevate and Vitas's behavior with the human tester. We draw the venn diagram to show the intersection state space between Elevate, Vitas and Manual testing. The text on each part represents the state space coverage rate when the unique states achieved by Elevate, Vitas and Manual testing is set as the total space. The result is shown below. Compared with Vitas, Elevate covers more state space discovered by manual testing, which proves that Elevate works more like a manual tester. This result is in line with our expectations, as Elevate's three agents commit to complementing the semantic loss during VUI testing. To further analyze the characteristics of states achieved by Elevate and Vitas, we extract meaningful states, which represent the normal behavior of skills. Meaningful states are extracted manually from the states achieved by Elevate and Vitas. The venn diagram of meaningful states coverage by Elevate and Vitas is shown below. The figure demonstrate that Elevate achieves more meaningful states than Vitas. It proves that input events generated by Elevate are more effective in exploring apps' functionalities.","title":"Analyze the feature of state space achieved by Elevate, Vitas and Manual testing"},{"location":"effectiveness/","text":"Effectiveness The benefit of embedding the behavior model into prompts Elevate is essentially a model-based testing(MBT) method. All agents work around the behavior model. Consequently, taking behavior model as the input is the requirement of tasks. If there is no behavior model guidance, the LLM will degenerate into a chatbot style tester. To prove the effectiveness of embedding the behavior model into prompts, we compare GPT-4 with and without behavior model as prompt. GPT-4 without behavior model is represented as GPT4(chatbot), and GPT-4 with behavior model in prompts is represented as GPT4(behavior_model). The semantic state coverage rate acquired by GPT4(chatbot), GPT4(behavior_model) and Vitas is shown below. The result demonstrates that the behavior model provides effective testing information and improves the coverage of pure LLM in over 30%. Besides, GPT-4 with behavior model as prompts has advantages over traditional MBT method Vitas. Impact of LLM variance on Elevate To study the impact of the LLM's different versions on Elevate's performance, we implement Elevate based on GPT4-turbo-2024-04-09 and compare to Elevate implemented on GPT4-1106-Preview (Elevate in the paper). The semantic state coverage rate of two Elevate's versions and baselines is shown below. The curves of coverage with interaction rounds acquired by two Elevate's versions are almost coincidental. They have both around 20% more coverage than Vitas, and over 35% more coverage than GPT4(chatbot). The result proves that LLMs' versions has no impact on Elevate's performance.","title":"Effectiveness"},{"location":"effectiveness/#effectiveness","text":"","title":"Effectiveness"},{"location":"effectiveness/#the-benefit-of-embedding-the-behavior-model-into-prompts","text":"Elevate is essentially a model-based testing(MBT) method. All agents work around the behavior model. Consequently, taking behavior model as the input is the requirement of tasks. If there is no behavior model guidance, the LLM will degenerate into a chatbot style tester. To prove the effectiveness of embedding the behavior model into prompts, we compare GPT-4 with and without behavior model as prompt. GPT-4 without behavior model is represented as GPT4(chatbot), and GPT-4 with behavior model in prompts is represented as GPT4(behavior_model). The semantic state coverage rate acquired by GPT4(chatbot), GPT4(behavior_model) and Vitas is shown below. The result demonstrates that the behavior model provides effective testing information and improves the coverage of pure LLM in over 30%. Besides, GPT-4 with behavior model as prompts has advantages over traditional MBT method Vitas.","title":"The benefit of embedding the behavior model into prompts"},{"location":"effectiveness/#impact-of-llm-variance-on-elevate","text":"To study the impact of the LLM's different versions on Elevate's performance, we implement Elevate based on GPT4-turbo-2024-04-09 and compare to Elevate implemented on GPT4-1106-Preview (Elevate in the paper). The semantic state coverage rate of two Elevate's versions and baselines is shown below. The curves of coverage with interaction rounds acquired by two Elevate's versions are almost coincidental. They have both around 20% more coverage than Vitas, and over 35% more coverage than GPT4(chatbot). The result proves that LLMs' versions has no impact on Elevate's performance.","title":"Impact of LLM variance on Elevate"},{"location":"experiment/","text":"Experiment Download experiment to do experiments. The elevate_10min, gpt4_chatbot_10min, vitas_10min show the communication logs for Elevate, GPT4(chatbot) and Vitas to test skills for 10 minutes. The ablation1, ablation2, ablation are for ablation studies. The elevate+llama2-70b-chat-hf_10min and llama2-70b-chat-hf_chatbot_10min are communication logs of Elevate-Llama2-70b-chat and Llama2-70b-chat(chatbot) respectively. The large_scale contains the communciation logs of skills in large-scale dataset. The user contains the commucation logs of our user study. The experiment.py file is the source code to do experiments. Study1 Figure 7 shows the comparison of sentence states and semantic states covered by Elevate, GPT4(chatbot) and Vitas. Figure 8 shows the average semantic state coverage rate with varying interaction rounds between Elevate and baselines. It is important to note that the results here show the communication logs of each tool testing 10 minutes. But in the study 1 we compare the semantic state coverage rate of each tool testing for the same number of interaction rounds. To get the semantic/sentence states and average semantic state coverage rate achieved by Elevate and the baselines, run: python experiment.py 1 The results will be shown in study1_1_.xlsx and study1_2.xlsx . Study2 Figure 9 shows the average semantic state coverage rate by Elevate, w/o States extraction, w/o Input events generation and w/o State space exploration. To get the raw data of the ablation study, run: python experiment.py 2 The result will be shown in study2.xlsx . Study3 Figure 10 shows the average semantic state coverage rate achieved by Elevate-Llama2-70b-chat, Vitas and Llama2-70b-chat(chatbot). To get their state coverage rate on the benchmark, run: python experiment.py 3 The result will be shown in study3.xlsx . Study4 Figure 11 shows the comparison of coverage rate achieved by Elevate and Vitas on 4000 skills from all categories. To get the coverage rate of Elevate and Vitas on these 4000 skills, run: python experiment.py 4 The result will be shown in study4.xlsx .","title":"Experiment"},{"location":"experiment/#experiment","text":"Download experiment to do experiments. The elevate_10min, gpt4_chatbot_10min, vitas_10min show the communication logs for Elevate, GPT4(chatbot) and Vitas to test skills for 10 minutes. The ablation1, ablation2, ablation are for ablation studies. The elevate+llama2-70b-chat-hf_10min and llama2-70b-chat-hf_chatbot_10min are communication logs of Elevate-Llama2-70b-chat and Llama2-70b-chat(chatbot) respectively. The large_scale contains the communciation logs of skills in large-scale dataset. The user contains the commucation logs of our user study. The experiment.py file is the source code to do experiments.","title":"Experiment"},{"location":"experiment/#study1","text":"Figure 7 shows the comparison of sentence states and semantic states covered by Elevate, GPT4(chatbot) and Vitas. Figure 8 shows the average semantic state coverage rate with varying interaction rounds between Elevate and baselines. It is important to note that the results here show the communication logs of each tool testing 10 minutes. But in the study 1 we compare the semantic state coverage rate of each tool testing for the same number of interaction rounds. To get the semantic/sentence states and average semantic state coverage rate achieved by Elevate and the baselines, run: python experiment.py 1 The results will be shown in study1_1_.xlsx and study1_2.xlsx .","title":"Study1"},{"location":"experiment/#study2","text":"Figure 9 shows the average semantic state coverage rate by Elevate, w/o States extraction, w/o Input events generation and w/o State space exploration. To get the raw data of the ablation study, run: python experiment.py 2 The result will be shown in study2.xlsx .","title":"Study2"},{"location":"experiment/#study3","text":"Figure 10 shows the average semantic state coverage rate achieved by Elevate-Llama2-70b-chat, Vitas and Llama2-70b-chat(chatbot). To get their state coverage rate on the benchmark, run: python experiment.py 3 The result will be shown in study3.xlsx .","title":"Study3"},{"location":"experiment/#study4","text":"Figure 11 shows the comparison of coverage rate achieved by Elevate and Vitas on 4000 skills from all categories. To get the coverage rate of Elevate and Vitas on these 4000 skills, run: python experiment.py 4 The result will be shown in study4.xlsx .","title":"Study4"},{"location":"weaknesses/","text":"Weaknesses Number of weaknesses detected by Elevate and Vitas on the benchmark The benchmark contains available and stable Alexa skills. However, they also suffer from problems including unexpected exit, privacy violation and unstoppable problems. While Elevate and baselines tested these skills, they recorded these problems. The number of weaknesses found by Elevate and baselines with time is shown below. The number of weaknesses detected with time is proportional to the increase of the state space with interaction rounds. We notice that weaknesses found by Elevate are fewer than baselines in the first two minutes. The reason is that Generator tends to generate meaningful inputs and Planner prefers to choose the context-related input, which are closer to natural language (see coverage features ). These context-related inputs help us dig out skills' deeper behavior. After multiple rounds of effective interactions, Elevate can find more hidden problems. As a comparison, the inputs chose by baselines lack context relavance or the natural language format. As skills have difficulty understanding these inputs, they are more likely to expose problems. However, as these inputs cannot help find deeper functionalities, baselines continue to discover shallow problems repeatly. What's more, as users rarely deliberately say strange words during use, problems found by baselines are less likely to happen in practice. Weaknesses found by Elevate on 4000 skills Vitas summarized five common problems on VPA apps: unexpected exit, privacy violation, unable to exit, expected started apps and unavailable apps. Elevate supports checking these problems while exploring VPA apps' behavior. To show Elevate's applicability on real-world apps, Elevate tested the 4000 skills in the large-scale dataset. The information of these 4000 skills comes from Vitas's benchmark, but the skills under test is the latest version on the Alexa skill store. The number of apps with various problems found by Elevate is shown below. category unexpected exit privacy violation unable to exit expected started apps unavailable apps Business_Finance 121 11 13 6 55 Connected_Car 5 1 2 0 8 Education_Reference 209 8 8 25 105 Food_Drink 123 5 7 3 39 Games_Trivia 207 7 12 27 230 Health_Fitness 62 2 0 8 39 Kids 37 1 0 14 117 Lifestyle 199 10 5 18 118 Local 11 0 0 0 6 Movies_TV 11 0 1 2 8 Music_Audio 2 0 0 0 0 News 87 0 3 6 41 Novelty_Humor 436 7 6 29 138 Productivity 85 7 16 9 59 Shopping 28 4 3 1 21 Smart_Home 90 2 36 9 263 Social 85 5 5 14 72 Sports 22 1 0 7 30 Travel_Transportation 61 2 7 3 44 Utilities 81 5 7 9 47 Weather 46 3 3 4 20 Total 2008 81 134 194 1460 Surprisingly, most of skills are detected with problems. The problems found by Elevate cover those discovered by Vitas. In addition, Elevate found over a half of skills suffer from unexpected exit. Approximately 2% of skills have privacy violation issues. Applicability on top real-world apps To further prove that our method has applicability and generality, we choose the top 60 skills with most rating number (util February, 2024). The below table shows the number of real-world apps with problems found by Elevate. problem type unexpected exit privacy violation unable to exit expected started apps unavailable apps number 36 2 3 4 14 The result shows that Elevate found at least 59.0% of apps with unexpected exit and 3.3% of apps with privacy violation. Even apps with a large number of users suffer from various problems. This finding not only proves the applicability of Elevate, but also draws our attention to the quality of VPA apps. Besides, there are 31.1% of apps that cannot be correctly visited, including expected started apps and unavailable apps. Skills' iteration is quite fast as almost one-third of apps cannot launch within ten months.","title":"Weaknesses"},{"location":"weaknesses/#weaknesses","text":"","title":"Weaknesses"},{"location":"weaknesses/#number-of-weaknesses-detected-by-elevate-and-vitas-on-the-benchmark","text":"The benchmark contains available and stable Alexa skills. However, they also suffer from problems including unexpected exit, privacy violation and unstoppable problems. While Elevate and baselines tested these skills, they recorded these problems. The number of weaknesses found by Elevate and baselines with time is shown below. The number of weaknesses detected with time is proportional to the increase of the state space with interaction rounds. We notice that weaknesses found by Elevate are fewer than baselines in the first two minutes. The reason is that Generator tends to generate meaningful inputs and Planner prefers to choose the context-related input, which are closer to natural language (see coverage features ). These context-related inputs help us dig out skills' deeper behavior. After multiple rounds of effective interactions, Elevate can find more hidden problems. As a comparison, the inputs chose by baselines lack context relavance or the natural language format. As skills have difficulty understanding these inputs, they are more likely to expose problems. However, as these inputs cannot help find deeper functionalities, baselines continue to discover shallow problems repeatly. What's more, as users rarely deliberately say strange words during use, problems found by baselines are less likely to happen in practice.","title":"Number of weaknesses detected by Elevate and Vitas on the benchmark"},{"location":"weaknesses/#weaknesses-found-by-elevate-on-4000-skills","text":"Vitas summarized five common problems on VPA apps: unexpected exit, privacy violation, unable to exit, expected started apps and unavailable apps. Elevate supports checking these problems while exploring VPA apps' behavior. To show Elevate's applicability on real-world apps, Elevate tested the 4000 skills in the large-scale dataset. The information of these 4000 skills comes from Vitas's benchmark, but the skills under test is the latest version on the Alexa skill store. The number of apps with various problems found by Elevate is shown below. category unexpected exit privacy violation unable to exit expected started apps unavailable apps Business_Finance 121 11 13 6 55 Connected_Car 5 1 2 0 8 Education_Reference 209 8 8 25 105 Food_Drink 123 5 7 3 39 Games_Trivia 207 7 12 27 230 Health_Fitness 62 2 0 8 39 Kids 37 1 0 14 117 Lifestyle 199 10 5 18 118 Local 11 0 0 0 6 Movies_TV 11 0 1 2 8 Music_Audio 2 0 0 0 0 News 87 0 3 6 41 Novelty_Humor 436 7 6 29 138 Productivity 85 7 16 9 59 Shopping 28 4 3 1 21 Smart_Home 90 2 36 9 263 Social 85 5 5 14 72 Sports 22 1 0 7 30 Travel_Transportation 61 2 7 3 44 Utilities 81 5 7 9 47 Weather 46 3 3 4 20 Total 2008 81 134 194 1460 Surprisingly, most of skills are detected with problems. The problems found by Elevate cover those discovered by Vitas. In addition, Elevate found over a half of skills suffer from unexpected exit. Approximately 2% of skills have privacy violation issues.","title":"Weaknesses found by Elevate on 4000 skills"},{"location":"weaknesses/#applicability-on-top-real-world-apps","text":"To further prove that our method has applicability and generality, we choose the top 60 skills with most rating number (util February, 2024). The below table shows the number of real-world apps with problems found by Elevate. problem type unexpected exit privacy violation unable to exit expected started apps unavailable apps number 36 2 3 4 14 The result shows that Elevate found at least 59.0% of apps with unexpected exit and 3.3% of apps with privacy violation. Even apps with a large number of users suffer from various problems. This finding not only proves the applicability of Elevate, but also draws our attention to the quality of VPA apps. Besides, there are 31.1% of apps that cannot be correctly visited, including expected started apps and unavailable apps. Skills' iteration is quite fast as almost one-third of apps cannot launch within ten months.","title":"Applicability on top real-world apps"}]}